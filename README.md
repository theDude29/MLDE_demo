# MLDE_demo

On s'interesse dans ce notebook à la modélisation de phénomènes physiques via une approche combiné ML + equation differentielle. En effet, modéliser un phénomène uniquement via des équations différentielles mène à des prédictions médiocres puisque nous ne pouvons modéliser tous les phénomènes physique ayant lieu et qu'il y a des erreurs numériques dans la résolution de ces dernières. Cependant, faire des prédictions uniquement basées sur du ML reviendrait à demander à un réseaux de neuronne de re apprendre les équations de la physique dont on connait déjà une partie... Ainsi une approche ML + equadiff permet de tirer avantage de nos connaissances physique tout en laissant au réseau de neuronnes le soin d'apprendre les détails que l'on ne connait pas.

On s'interessera plus particulièrement dans ce notebook à l'exemple jouet suivant: on suppose que l'on observe des trajectoires (réalisées à partir de l'équation $\ddot x + a \dot x + b x = 0$) dont on connait un modèle physique médiocre (on suppose que l'on ne parvient pas à faire mieux que de proposer le modèle $\ddot x + b x = 0$ pour justifier nos observations...) mais malin comme nous somme on utilise un réseau de neuronne pour améliorer notre modèle (on cherche à trouver $\theta$ tel que $\ddot x + b x = F_{\theta}(x,\dot x, t) \ (*)$ soit une bonne modélisation du problème... (idéalement on veut donc que le réseau de neuronnes comprenne que $F_{\theta}(x,\dot x, t) = -a \dot x$))

La difficulté ici pour l'entrainement du réseau de neuronnes est de pouvoir calculer la dérivé de la solution de l'équation $(*)$ par rapport à $\theta$. Heureusement la bibliothèque JAX (https://docs.jax.dev/en/latest/index.html) fournit des outils pour calculer facilement des gradients via des méthodes d'autodiff (cf ce papier qui fait un tour d'horizon des méthodes de différenciation automatique implémentées dans JAX : https://arxiv.org/pdf/1502.05767). Il faut ensuite que le schéma d'intégration soit implémenté au dessus de JAX pour pouvoir bénéficier de toute la puissance de l'autodiff. C'est ce qui est fait dans la librairie Diffrax (https://docs.kidger.site/diffrax/). Enfin pour des raisons pratique on utilise FLAX (https://flax.readthedocs.io/en/stable/index.html) pour construire le réseau de neuronnes.

Le principe de ce que l'on fait ici est donc le même que celui utilisé dans https://arxiv.org/pdf/2311.07222 mais on a remplacé les observations météo par des trajectoires de $\ddot x + a \dot x + b x = 0$, nos connaissances des équations d'évolution de la météo par $\ddot x + b x = 0$ et le réseau de neuronnes utilisé par leur équipe par $F_{\theta}$...
